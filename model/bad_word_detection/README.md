비속어 탐지 모델
- Outline
    욕설 키워드 기반 크롤링(네이버 뉴스, 카페, 블로그, 디씨인사이드, 네이트 뉴스)
    STT변환을 통해 나올 수 있는 단어(완전한 글자) 위주로 학습(약 ? 개 라벨링)
    자모분리를 통한 fasttext word embedding vocab구성
    RandomForest: accuracy: 약 ? 퍼, f1-score: 약? 퍼
    1DCNN: accuracy: 약 ?퍼, f1-score: 약?퍼
    
- Process 
    수집 -> 점검 및 탐색 -> 전처리 및 정제 -> 모델링 및 훈련 -> 평가 
    
    1) 수집 (Acquisition)
    - 말뭉치 또는 코퍼스(corpus) : 조사나 연구 목적에 의해서 특정 도메인으로부터 수집된 텍스트 집합, 텍스트 데이터의 파일 형식은 txt파일, csv파일, xml파일 등, 웹 수집기를 통해 수집된 데이터, 영화 리뷰 등

    2) 점검 및 탐색 (Inspection and exploration)
    - 데이터의 구조, 노이즈 데이터, 머신 러닝 적용을 위해서 데이터를 어떻게 정제해야하는지 등을 파악
    - 탐색적 데이터 분석(Exploratory Data Analysis, EDA) 단계 : 독립 변수, 종속 변수, 변수 유형, 변수의 데이터 타입 등을 점검하며 데이터의 특징과 내재하는 구조적 관계를 알아내는 과정

    3) 전처리 및 정제 (Preprocessing and Cleaning)
    - 토큰화, 정제, 정규화, 불용어 제거 등의 단계를 포함

    4) 모델링 및 훈련 (Modeling and Training)
    - 적절한 머신 러닝 알고리즘을 선택하여 모델링이 끝났다면, 전처리가 완료 된 데이터를 머신 러닝 알고리즘을 통해 기계에 학습(training) 시킨다. 이를 훈련이라고 한다. 
    두 용어를 혼용해서 사용 (학습 = 훈련)
    훈련 후 우리가 원하는 자연어 처리 작업을 수행할 수 있게 된다. 
    * 주의사항 : 모든 데이터를 학습시키면 안된다. 머신 러닝, 딥 러닝에서 데이터 중 일부는 테스트용으로 남겨두고 훈련용 데이터만 훈련에 사용해야 한다. 
    그래야 기계가 학습을 하고나서, 현재 성능 측정 가능, 과적합(overfitting) 상황을 막을 수 있다. 데이터의 양이 충분하면 훈련용, 검증용, 테스트용 세 가지로 나누고 훈련용 데이터만 훈련에 사용 (예: 훈련용=학습지, 검증용=모의고사, 테스트용=수능 시험)
    현업의 경우 검증용 데이터는 필수적! why? 훈련용 데이터가 제대로 학습이 되었는지 판단 가능, 검증용 데이터를 사용하여 모델의 성능 개선에 사용, 테스트용 데이터는 모델의 최종 성능 평가하는 데이터로 모델의 성능 수치화하여 평가하기 위해 사용 

    5) 평가 (Evaluation)
    - 학습 후 테스트용 데이터로 성능 평가, 평가 방법은 기계가 예측한 테스트용 데이터의 실제 정답과 얼마나 가까운지를 측정


    EDA -> FastTextVocab -> TrigramVectorize -> 1DCNN or RandomForest -> Test
    문장에서 정규식표현으로 욕설이 나오는 부분 추출
    추출된 어절 중심으로 좌우 단어 trigram 반환 ex) (나는, 바보, 멍청이, 3) 3번째위치에 바보가 있고 좌우어절은 나는, 멍청이 이다
    trigram을 fasttext embedding model을 활용하여 vectorize
    vectorize된 데이터를 Random Forest or 1DCNN Model에 넣어 예측
    

- Test
    Pretrained 모델로 예측해보기
   

- vocab 시각화
    vocab 2차원으로 임베딩 후 plot

- 유사한 단어들 뽑아보기

- 모델 결과
    1DCNN
    Random Forest




패스트텍스트(FastText)
단어를 벡터로 만드는 또 다른 방법으로는 페이스북에서 개발한 FastText가 있습니다. Word2Vec 이후에 나온 것이기 때문에, 메커니즘 자체는 Word2Vec의 확장이라고 볼 수 있습니다. Word2Vec와 FastText와의 가장 큰 차이점이라면 Word2Vec는 단어를 쪼개질 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주합니다. 즉 내부 단어(subword)를 고려하여 학습합니다.